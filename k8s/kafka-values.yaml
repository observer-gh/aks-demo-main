# kafka-values.yaml
fullnameOverride: team-kafka
kraft:
  enabled: true

controller:
  replicaCount: 3
  resources:
    requests: { cpu: "100m", memory: "256Mi" }
    limits: { cpu: "200m", memory: "512Mi" }
  persistence:
    enabled: true
    size: 5Gi
  podAntiAffinityPreset: soft

  # optional: in case k8s fail to spread evenly across nodes
  topologySpreadConstraints:
    - maxSkew: 1 # prevent all controller on same node
      topologyKey: kubernetes.io/hostname # node names
      whenUnsatisfiable: ScheduleAnyway # but still schedule even if contraint not met
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: kafka
          app.kubernetes.io/component: controller
  podSecurityContext: { fsGroup: 1001 }
  volumePermissions: { enabled: true }

broker:
  replicaCount: 1 # only 1 broker
  resources:
    requests: { cpu: "300m", memory: "1Gi" }
    limits: { cpu: "500m", memory: "1.5Gi" }
  persistence:
    enabled: true
    size: 20Gi

  # override default bitnami settings to fit inside tight resource
  configurationOverrides:
    default.replication.factor: "1"
    min.insync.replicas: "1"
    offsets.topic.replication.factor: "1"
    transaction.state.log.replication.factor: "1"
    transaction.state.log.min.isr: "1"
    num.partitions: "6"
    log.retention.hours: "12"
  podAntiAffinityPreset: soft
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: ScheduleAnyway
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: kafka
          app.kubernetes.io/component: broker
  podSecurityContext: { fsGroup: 1001 }
  volumePermissions: { enabled: true }

service: { type: ClusterIP }
auth:
  clientProtocol: plaintext # TEST ONLY
  interBrokerProtocol: plaintext
metrics:
  jmx: { enabled: false }
  kafka: { enabled: false }
